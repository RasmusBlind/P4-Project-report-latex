% Chapter 'Evaluation'
In this chapter we will evaluate the performance of our transcription system. bla bla

\section{Methods}

	\subsection{Training and Test Sets}
		The dataset consists of sound segments, segmented based on the annotations. This means that our segmentation of sound (present in our application), will not be part of the evaluation.
		The training and test sets of sounds for the KNN classifier are randomly chosen from the same pool (the dataset). It is distributed between the training and test set in a 70\%/30\% ratio, accordingly, for each class. This means there will not be a fixed number of sounds for each class (neither total nor divided), but rather a fixed distribution between the number of training and test sounds for each class. We can do this instead of e.g. k-fold cross validation, due to scale of our collected dataset. The ratio was suggested by our supervisor\footnote{Bob L. Sturm}. The composition of the entire dataset has been summarized in table \ref{table:eval:datasetComposition}. 
		Furthermore, all sounds with a duration less than the windowsize used for feature calculation, are removed before testing. This is done before splitting the dataset, such as to make sure we do not distort the 70/30 distribution.

		\begin{table}
			\centering
			\begin{tabular}{|l|r|r|}
				\hline
				Value  &  Count  & Percent \\ \hline
		      noise    &  150    & 10.19\% \\ \hline
		          k    &  466    & 31.66\% \\ \hline
		  undefined    &  130    &  8.83\% \\ \hline
		          s    &  331    & 22.49\% \\ \hline
		         hh    &  395    & 26.83\% \\ \hline
		      TOTAL    &  1472	 & 100.00\% \\ \hline

			\end{tabular}
			\caption{Dataset composition}
			\label{table:eval:datasetComposition}
		\end{table}


	\subsection{Variables}
		As the number of combinations of features, and parameters of features and classifiers, is very large, we purposely keep as few variables and as many constants as possible. We chose not to test combinations of features, but rather single features one at a time.
		The primary variable to be tested is the K in the KNN classifier (how many neighbors it considers). Some changes will be made to feature parameters. Each feature will be tested with a couple of different sets of parameters:
		
		\begin{itemize}
			\item MFCC
				\begin{itemize}
					\item 20ms windows, 10ms skip
					\item 10ms windows, 5ms skip
					\item 5ms windows, 2ms skip
				\end{itemize}
			
			\item RMS
				\begin{itemize}
					\item has no parameters, so only 1 test
				\end{itemize}
				
			\item ZCR
				\begin{itemize}
					\item has no parameters, so only 1 test
				\end{itemize}
		
		\end{itemize}
		
		All features will be calculated for $K \in [1;10]$.


	\subsection{Measures}
		A confusion table is created for each test (each unique combination of variables). This will be shown in percentages (or rather, values between 0 and 1). Overall accuracy is calculated, along with precision, recall, and F-score for each class individually. For the sake of compactness, all the measures are included in an extended confusion table, as shown in the explanatory table \ref{table:eval:explanatory}. 
		The most important measure, for the sake of measuring our transcription system, would be the precision, that is, the amount of correctly transcribed sounds over the actual amount of that sound class.
		Recall should just be ignored, since we only test on the dataset, which means that we always test on all available samples of any given class, meaning that it would be the same as the amount of . This would be different, had the segmentation been part of the evaluation. It is included nonetheless, as further progress with this project might find a need for it.

			\begin{table}
				\centering
				\begin{tabular}{|c | c | c | c | c |}
					\hline
					 & Real Class(1) & Real Class(2) & Real Class(3) & Precision\\ \hline
					Label(1)  & ... & ... & ... & ...\\ \hline
					Label(2)  & ... & ... & ... & ...\\ \hline
					Label(3) & ... & ... & ... & ...\\ \hline
					Recall   & ... & ... & ... & \multicolumn{1}{c}{Overall}\\ \cline{1-4}
					F-Score & ... & ... & ... & \multicolumn{1}{c}{Accuracy} \\ \cline{1-4}
				\end{tabular}
				\caption{$K=1$}
				\label{table:eval:explanatory}
			\end{table}
		chi squared?
		sign test?
		
		
	\subsection{Test Implementation}		% REFERENCES TO APPENDIX!
		To ease the testing of the transcription system, two additional scripts were created: testTables.m and testPlots.m.
		The testTables.m script simply runs a test and formats the results in tables usable for \LaTeX, while testPlots.m creates plots for precision, recall, and F over K, including the overall accuracy in all three plots, and saves them as EPS images\footnote{Encapsulated Postscript format - it keeps plots from Matlab sharp and pretty when imported in \LaTeX}. Both scripts are relatively simple and well-commented, and thus should require no more than than a simple mention, as they do not include features not already explained in chapter \ref{chapter:OurApproach}.
			
 
\section{Results}
For all results found, some interpretation of the data and statistics will be presented. Further discussion about the relevance of the data, possible mistakes that affected results, or similar, will be covered in chapter \ref{chapter:Discussion}. 
Confusion tables take up a lot of pages, so any tables not directly relevant can be found in appendix \ref{APPENDIXBORWHATEVER}.


	\subsection{MFCC}
		\input{testMFCCPlots.tex}
		
		Three different configurations of of the MFCC feature have been tested for $K \in [1;10]$: 20ms,10ms, and 5ms window sizes with 10ms, 5ms, and 2ms window skip respectively. All tests use feature vectors consisting of the first 20 coefficients. 
		The best results were shown around $K=3$ and $K=5$. A top 
		

		% REMEMBER REMEMBER THE 5TH OF, JUST ADD THE DAMN 'tex/' AT SOME POINT	
		\input{testMFCC20ms10ms.tex}
		\input{testMFCC10ms5ms.tex}
		\input{testMFCC5ms2ms.tex}	
	